# Seq2Seq Model Architecture
#
# - Run full model training run with Pande et al. hyperparameters to test EarlyStop callback and monitor unthrottled
#   test performance metrics.
#
# Training run metrics:
#   - Epochs: 50 (28 run as EarlyStopping callback triggered)
#
#   - Train Loss (Smoothed): 0.0431
#   - Train Accuracy (Smoothed): 0.692
#
#   - Validation Loss (Smoothed): 0.101
#   - Validation Accuracy (Smoothed): 0.682
#
#   - Test Loss (Smoothed): 0.103
#   - Test Accuracy (Smoothed): 0.682
#
# Training Performance Evaluation
# 1. Training vs Validation Accuracy:
#     - Train Accuracy: 0.692
#     - Validation Accuracy: 0.682
#     - Difference: -0.010 (-1.44%)
#
#   A small gap of 1.44% between training and validation accuracy suggests the model's ability to correctly classify
#   samples is consistent across both datasets. This indicates good generalization, with no significant overfitting.
#
# 2. Training vs Validation Loss:
#     - Train Loss: 0.0431
#     - Validation Loss: 0.101
#     - Difference: +0.0579 (+134%)
#
#   A larger gap of ~134% between training and validation loss is less ideal, but doesn't necessarily indicate
#   overfitting as accuracy metrics are tightly aligned. Further investigation is needed to ascertain the reason for
#   this loss discrepancy.
#   The discrepancy between training and validation loss is smaller than model V13, indicating that the current
#   model architecture isn't able to fully harness the potential of a deeper sequential model.
#
# 3. Validation vs Testing Accuracy and Loss:
#   - Validation Accuracy: 0.682
#   - Testing Accuracy: 0.682
#   - Difference: 0.00 (0.00%)
#
#   - Validation Loss: 0.101
#   - Testing Loss: 0.103
#   - Difference: +0.002 (+1.98%)
#
# Consistent test performance compared to validation indicates good generalization and low overfitting.
#
# Compared to model V13:
#   - Both loss and accuracy for training, testing and validation are comparable, with model V14 showing marginal
#     improvement in training vs validation metrics, and validation vs testing metrics.
#   - Convergence is similar for both models, however the reduced number of layers for model V14 resulted in a reduction
#     in epoch time length of 1/2 (~1 h vs ~2h for model V131).
#   - Conclusion: Maintain encoder number of layers at 2 as per Liu et al. model.

# Data Configuration and Hyperparameters
data:
  products_file: 'data/preprocessed/liu-et-al/products_smiles'
  reactants_file: 'data/preprocessed/liu-et-al/reactants_smiles'
  products_valid_file: 'data/preprocessed/liu-et-al/validation_products_smiles'
  reactants_valid_file: 'data/preprocessed/liu-et-al/validation_reactants_smiles'
  tokenizer_save_path: 'data/training/liu-et-al/model-v14/tokenizer/model_v14_tokenizer.json'
  max_encoder_seq_length: 140
  max_decoder_seq_length: 140
  batch_size: 32
  test_size: 0.3
  random_state: 4

# Model Configuration and Hyperparameters
model:
  input_vocab_size: null  # To be set dynamically based on tokenizer
  output_vocab_size: null  # To be set dynamically based on tokenizer
  attention_dim: 512
  encoder_embedding_dim: 256
  decoder_embedding_dim: 512
  units: 512
  weight_decay: null
  dropout_rate: 0.2
  learning_rate: 0.0001
  metrics: ['accuracy']

# Training Configuration and Hyperparameters
training:
  epochs: 50
  patience: 5
  model_save_path: 'data/training/liu-et-al/model-v14/model'
  test_metrics_dir: 'data/training/liu-et-al/model-v14/evaluate'
  log_dir: 'logs/liu-et-al/model-v14'
  checkpoint_dir: 'data/training/liu-et-al/model-v14/checkpoints'
  num_samples: null # Number of samples to use for debugging model

# Environment Configuration
env:
  determinism:
    python_seed: "44478977"
    random_seed: 440651
    numpy_seed: 110789
    tf_seed: 61592