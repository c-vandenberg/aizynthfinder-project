# Seq2Seq Model Architecture
# - Changed loss function to `losses.losses.MaskedSparseCategoricalCrossentropy`
# - This has resulted in an increase in loss function for the following reasons:
#   1. Previous loss function (`models.utils.Seq2SeqModelUtils.masked_sparse_categorical_crossentropy) was computing
#      the loss for each token in the batch, and masking the loss for padding tokens by setting them to zero.
#   2. As a result, it was computing a mean loss over all tokens, **including zeros from padding tokens**.
#   3. By averaging over the entire sequence length (including the zeros from padding tokens), the **overall loss
#      was artificially reduced**.
#   4. The new `losses.losses.MaskedSparseCategoricalCrossentropy` loss function only calculates the mean loss over
#      non-padding tokens, resulting in a worse, but more accurate, loss function value.
#
# Training run metrics:
#   - Epochs: 5
#
#   - Train Loss (Smoothed): 0.640
#   - Train Accuracy (Smoothed): 0.561
#
#   - Validation Loss (Smoothed): 0.551
#   - Validation Accuracy (Smoothed): 0.577
#
#   - Test Loss (Smoothed): 0.444
#   - Test Accuracy (Smoothed): 0.600

# Data Configuration and Hyperparameters
data:
  products_file: 'data/preprocessed/pande-et-al/products_smiles'
  reactants_file: 'data/preprocessed/pande-et-al/reactants_smiles'
  products_valid_file: 'data/preprocessed/pande-et-al/validation_products_smiles'
  reactants_valid_file: 'data/preprocessed/pande-et-al/validation_reactants_smiles'
  tokenizer_save_path: 'data/training/pande-et-al/model-v8/tokenizer/model_v8_tokenizer.json'
  max_encoder_seq_length: 140
  max_decoder_seq_length: 140
  batch_size: 32
  test_size: 0.3
  random_state: 4

# Model Configuration and Hyperparameters
model:
  input_vocab_size: null  # To be set dynamically based on tokenizer
  output_vocab_size: null  # To be set dynamically based on tokenizer
  attention_dim: 256
  encoder_embedding_dim: 128
  decoder_embedding_dim: 256
  units: 256
  dropout_rate: 0.2
  learning_rate: 0.0001
  metrics: ['accuracy']

# Training Configuration and Hyperparameters
training:
  epochs: 5
  patience: 5
  model_save_path: 'data/training/pande-et-al/model-v8/model'
  log_dir: 'logs/pande-et-al/model-v8'
  checkpoint_dir: 'data/training/pande-et-al/model-v8/checkpoints'
  num_samples: null # Number of samples to use for debugging model

# Environment Configuration
env:
  determinism:
    python_seed: "44478977"
    random_seed: 440651
    numpy_seed: 110789
    tf_seed: 61592