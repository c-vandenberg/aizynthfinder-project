# Seq2Seq Model Architecture:
# - Identical architecture to model V2, except for following change:
#   * Reverse Source Sequence before Encoding:
#     - Analysis of paper by Liu et al shows that they reverse the source sequence (the SMILES string of the target
#       molecule) before feeding it into the encoder.
#     - This technique can help improve the alignment between the input and output, especially in cases where the
#       dependencies are more at the end of the input sequence. This was originally introduced in neural machine
#       translations and has shown benefits in sequence-to-sequence tasks.
#   * Switching to Character-Wise Encoding:
#       - Analysis of paper by Liu et al shows that they use character-wise encoding of SMILES strings, meaning that
#         each character in the SMILES notation is treated as a separate token.
#       - Character-wise encoding captures the fine-grain structure of SMILES strings, allowing the model to learn
#         detailed patterns and potentially reduce vocabulary size.
#
# Training run metrics:
#   - Train Loss (Smoothed): 0.280
#   - Train Accuracy (Smoothed): 0.607
#   - Validation Loss (Smoothed): 0.256
#   - Validation Accuracy (Smoothed): 0.614
#   - Test Loss (Smoothed): 0.239
#   - Test Accuracy (Smoothed): 0.620

# Data Configuration and Hyperparameters
data:
  products_file: 'data/preprocessed/pande-et-al/products_smiles'
  reactants_file: 'data/preprocessed/pande-et-al/reactants_smiles'
  products_valid_file: 'data/preprocessed/pande-et-al/validation_products_smiles'
  reactants_valid_file: 'data/preprocessed/pande-et-al/validation_reactants_smiles'
  tokenizer_save_path: 'data/training/pande-et-al/model-v3/tokenizer/model_v3_tokenizer.json'
  max_encoder_seq_length: 140
  max_decoder_seq_length: 140
  batch_size: 32
  test_size: 0.3
  random_state: 4

# Model Configuration and Hyperparameters
model:
  input_vocab_size: null  # To be set dynamically based on tokenizer
  output_vocab_size: null  # To be set dynamically based on tokenizer
  embedding_dim: 256
  units: 256
  dropout_rate: 0.2
  learning_rate: 0.0001
  metrics: ['accuracy']

# Training Configuration and Hyperparameters
training:
  epochs: 10
  patience: 5
  model_save_path: 'data/training/pande-et-al/model-v3/model'
  log_dir: 'logs/pande-et-al/model-v3'
  checkpoint_dir: 'data/training/pande-et-al/model-v3/checkpoints'
  num_samples: null # Number of samples to use for debugging model

# Environment Configuration
env:
  determinism:
    python_seed: "44478977"
    random_seed: 440651
    numpy_seed: 110789
    tf_seed: 61592