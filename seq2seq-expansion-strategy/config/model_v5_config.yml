# Seq2Seq Model Architecture
# - Identical architecture to model V3, except for following change:
#   * Split encoder and decoder embedding dim to match the paper by Pande et al.
#   * They have both the encoder and decoder embedding dimensions at 512, however because the encoder is bidirectional,
#     each direction (forward and backward) has its own LSTM units.
#   * Therefore, we need to set the number of units per direction to 256, so the concatenated output will have a
#     dimension of 512 (256 units forward + 256 units backward).
#   * They also have attention dimension of 512, however the current model iteration does not have the functionality to
#     configure attention layer dimension. This will be done in a future iteration.
#

# Data Configuration and Hyperparameters
data:
  products_file: 'data/preprocessed/pande-et-al/products_smiles'
  reactants_file: 'data/preprocessed/pande-et-al/reactants_smiles'
  products_valid_file: 'data/preprocessed/pande-et-al/validation_products_smiles'
  reactants_valid_file: 'data/preprocessed/pande-et-al/validation_reactants_smiles'
  tokenizer_save_path: 'data/training/pande-et-al/model-v5/tokenizer/model_v5_tokenizer.json'
  max_encoder_seq_length: 140
  max_decoder_seq_length: 140
  batch_size: 32
  test_size: 0.3
  random_state: 4

# Model Configuration and Hyperparameters
model:
  input_vocab_size: null  # To be set dynamically based on tokenizer
  output_vocab_size: null  # To be set dynamically based on tokenizer
  attention_dim: 512
  encoder_embedding_dim: 256
  decoder_embedding_dim: 512
  units: 512
  dropout_rate: 0.2
  learning_rate: 0.0001
  metrics: ['accuracy']

# Training Configuration and Hyperparameters
training:
  epochs: 5
  patience: 5
  model_save_path: 'data/training/pande-et-al/model-v5/model'
  log_dir: 'logs/pande-et-al/model-v5'
  checkpoint_dir: 'data/training/pande-et-al/model-v5/checkpoints'
  num_samples: null # Number of samples to use for debugging model

# Environment Configuration
env:
  determinism:
    python_seed: "44478977"
    random_seed: 440651
    numpy_seed: 110789
    tf_seed: 61592